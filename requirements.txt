torch>=2.0.0
torchaudio>=0.12.0
torchvision>=0.15.0
scikit-learn>=1.3.0
numpy>=1.24.0
jiwer>=3.0.0
tensorboard>=2.13.0
wandb>=0.15.0
librosa>=0.10.0
soundfile>=0.12.0
pyyaml>=6.0
tqdm>=4.65.0
matplotlib>=3.7.0
seaborn>=0.12.0
pandas>=2.0.0
hydra-core>=1.3.0
omegaconf>=2.3.0
einops>=0.6.0
datasets>=2.12.0
transformers>=4.30.0
pytest>=7.3.0
black>=23.3.0
flake8>=6.0.0
mypy>=1.3.0
requests-toolbelt>=1.0.0
zstandard>=0.22.0
langsmith>=0.1.0

# ---- Optional performance / SOTA components ----
mamba-ssm>=1.1.0        # Official Selective State-Space Model implementation
flash-attn>=2.5.1       # Fused CUDA kernels for Transformer attention/FFN
faiss-cpu>=1.7.4        # Fast k-means (CPU build; GPU auto-enabled when CUDA present)
ctcdecode>=0.4          # Beam-search decoding (already optional in code)
phonemizer
scipy
