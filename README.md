# SerenaNet: A Multi-Modal Speech Recognition Architecture

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

SerenaNet is a state-of-the-art automatic speech recognition (ASR) model that combines multi-resolution temporal hierarchies, transformer encoders, and Mamba-based alignment for superior performance on multilingual speech recognition tasks.

## ğŸ¯ Goals

- Achieve WER 38.5 on Swahili, beating Samba-ASR (40.1)
- Support comprehensive ablation studies
- Stay under $2000 compute budget (~$150-$200 for experiments on Google Colab Pro+)
- Ensure complete originality with custom implementation

## ğŸ—ï¸ Architecture

SerenaNet consists of several key components:

- **Input Preprocessing**: Log-mel spectrograms (16 kHz, 25ms window, 10ms hop, 128 mel bins)
- **ATHM (Adaptive Temporal Hierarchy Module)**: Multi-resolution Conv1D with residual connections and MLP gating (~1.5M parameters)
- **Transformer Encoder**: 6-layer transformer (512 dim, 8 heads, ~72M parameters)
- **PESSL (Phoneme-Enhanced Self-Supervised Learning)**: k-means clustering with 100 clusters
- **CAR (CTC Alignment Refinement)**: Mamba-based state-space model (~0.3M parameters)
- **Decoder**: Linear layer for phoneme logits (~21k parameters)

## ğŸ“ Project Structure

```
SerenaArch2026/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/           # Model implementations
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ athm.py      # Adaptive Temporal Hierarchy Module
â”‚   â”‚   â”œâ”€â”€ transformer.py  # Transformer Encoder
â”‚   â”‚   â”œâ”€â”€ mamba.py     # Mamba SSM implementation
â”‚   â”‚   â”œâ”€â”€ car.py       # CTC Alignment Refinement
â”‚   â”‚   â”œâ”€â”€ decoder.py   # Output decoder
â”‚   â”‚   â”œâ”€â”€ pessl.py     # Phoneme-Enhanced SSL
â”‚   â”‚   â””â”€â”€ serenanet.py # Main model
â”‚   â”œâ”€â”€ data/            # Data processing
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ preprocessing.py  # Audio preprocessing
â”‚   â”‚   â”œâ”€â”€ augmentation.py   # SpecAugment
â”‚   â”‚   â”œâ”€â”€ datasets.py       # Dataset loaders
â”‚   â”‚   â””â”€â”€ phonemes.py       # Phoneme mappings
â”‚   â”œâ”€â”€ training/        # Training pipeline
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ trainer.py   # Main training loop
â”‚   â”‚   â”œâ”€â”€ losses.py    # Loss functions
â”‚   â”‚   â””â”€â”€ optimizer.py # Optimizer setup
â”‚   â”œâ”€â”€ evaluation/      # Evaluation metrics
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ metrics.py   # WER/PER calculations
â”‚   â”‚   â””â”€â”€ decoder.py   # Beam search decoder
â”‚   â””â”€â”€ utils/           # Utilities
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py    # Configuration handling
â”‚       â”œâ”€â”€ logger.py    # Logging setup
â”‚       â””â”€â”€ checkpoints.py  # Model checkpointing
â”œâ”€â”€ configs/             # Configuration files
â”‚   â”œâ”€â”€ base_config.yaml
â”‚   â”œâ”€â”€ pretrain_config.yaml
â”‚   â””â”€â”€ finetune_config.yaml
â”œâ”€â”€ experiments/         # Experiment scripts
â”œâ”€â”€ tests/              # Unit tests
â”œâ”€â”€ data/               # Raw and processed data
â”œâ”€â”€ checkpoints/        # Model checkpoints
â”œâ”€â”€ requirements.txt    # Python dependencies
â””â”€â”€ README.md          # This file
```

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
# Clone the repository
git clone <your-repo-url>
cd SerenaArch2026

# Create conda environment
conda create -n serenanet python=3.9
conda activate serenanet

# Install dependencies
pip install -r requirements.txt
```

### 2. Data Preparation

```bash
# Download and preprocess datasets
python scripts/prepare_data.py --config configs/base_config.yaml
```

### 3. Training

```bash
# Pre-training with PESSL
python scripts/train.py --config configs/pretrain_config.yaml

# Fine-tuning
python scripts/train.py --config configs/finetune_config.yaml
```

### 4. Evaluation

```bash
# Evaluate on test set
python scripts/evaluate.py --checkpoint checkpoints/best_model.pt --test_data data/test
```

## ğŸ§ª Ablation Studies

SerenaNet supports comprehensive ablation studies:

- No ATHM: Remove multi-resolution temporal hierarchy
- No PESSL: Remove phoneme-enhanced self-supervised learning
- CAR variants: Different Mamba configurations
- Single-resolution ATHM: Use only one temporal resolution

```bash
# Run ablation study
python experiments/ablation_study.py --config configs/ablation_config.yaml
```

## ğŸ’° Compute Budget

- Pre-training: ~100 hours (~$50)
- Fine-tuning: ~20 hours/dataset (~$20 for Swahili/LibriSpeech)
- Ablation studies: ~20 hours/test Ã— 6 tests (~$60)
- **Total: ~$130-$200, well under $2000 budget**

## ğŸ“Š Results

| Model | Swahili WER | LibriSpeech WER | Parameters |
|-------|-------------|-----------------|------------|
| Whisper-large | 42.3 | 3.2 | 1.55B |
| Wav2Vec2-large | 41.8 | 2.9 | 317M |
| Samba-ASR | 40.1 | 2.7 | 244M |
| **SerenaNet** | NA | NA | NA |

## ğŸ§ª Testing

```bash
# Run unit tests
pytest tests/

# Run specific test
pytest tests/test_models.py::test_athm
```

## ğŸ“ Citation

If you use SerenaNet in your research, please cite:

```bibtex
@article{serenanet2024,
  title={SerenaNet: Multi-Modal Speech Recognition with Adaptive Temporal Hierarchies},
  author={Chengjui Fan},
  journal={NA},
  year={2026}
}
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ“ Contact

- Email: osmond91349@outlook.com
- GitHub: [@CelsiaSolaraStarflare](https://github.com/CelsiaSolaraStarflare)